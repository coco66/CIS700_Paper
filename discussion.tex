In this project, we implemented HRED and DRL-SEQ2SEQ model with the OpenSubtitle Dataset. Unlike the results presented in the original paper of DRL-SEQ2SEQ \cite{Li}, our mutual information model generated poor responses and we could see that the model quickly diverges to wrong outcomes during training. There are several possible reasons for this poor performance. First, we did not use the curriculum strategy, but the strategy may play a critical role in RL learning since the reward functions do not perfectly describe quality of responses. Second, we used 0.5 for the learning rate of SGD (the default value in the tensorflow seq2seq model), but it may have been too large because the performance changed very quickly. Lastly, not using the baseline strategy may also have affected the performance since it is known that the strategy reduces the variance of PGO.